# DRL_mxnet
我比较喜欢用mxnet，最近看了莫烦大神的RL教程，简单的用mxnet代替了tensorflow实现了部分代码。非常感谢莫烦的教程，想了解更多深度强化学习的同学可以去观看莫烦的视频教程。^.^
有空我会继续用mxnet实现其它改进算法（感觉都挺像的，哈哈）


算法与莫烦的略有区别，OpenAI_gym中求误差的时候并没有做到下面这个例子一样
        """
        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:
        q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        q_target = q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:
        比如在:
            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;
            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:
        q_target =
        [[-1, 2, 3],
         [4, 5, -2]]

        所以 (q_target - q_eval) 就变成了:
        [[(-1)-(1), 0, 0],
         [0, 0, (-2)-(6)]]

        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.
        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.
        我们只反向传递之前选择的 action 的值,
        """
 (q_target - q_eval) 没有变成了:
        [[(-1)-(1), 0, 0],
         [0, 0, (-2)-(6)]]
 因为发现使用上面的方法更难收敛，具体原因还有待研究。。
 
 

github小白，有时间学习学习如何管理。。。。。

